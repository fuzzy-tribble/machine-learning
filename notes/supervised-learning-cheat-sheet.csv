Algorithm,Ideal Dataset,Less Ideal Dataset,Interpretability,Training Time,Pros,Cons,Categorical/Numerical Ratio Handling,Missing Value Tolerance
Linear Regression,"Low F/N ratio, linear relationships, few outliers","High F/N ratio, non-linear relationships, heteroscedasticity",High,"Very Fast (O(n·p²) with normal eq., O(n·p) with SGD)","Simple, interpretable, requires little tuning","Poor with non-linearity, sensitive to outliers","Best with numerical, needs encoding for categorical",Low - requires imputation
Logistic Regression,"Binary classification, linearly separable classes, low F/N","Non-linear class boundaries, many features",High,Very Fast (O(n·p) per iteration with SGD),"Probabilistic outputs, interpretable coefficients","Can't handle complex boundaries, limited to linear","Handles both, but categorical must be encoded",Low - requires imputation
Decision Tree,"Mixed feature types, non-linear relationships","Very small datasets, high F/N ratio (prone to overfitting)",Medium-High,Fast (O(n·log(n)·p)),"Handles non-linearity, categorical data","Overfits easily, unstable",Handles both naturally,Medium - can handle with surrogate splits or pruning
Random Forest,"Large datasets, complex relationships, high-dimensional data",Very high dimensional with low samples,Low-Medium,"Medium (O(m·n·log(n)·p), m = number of trees)","Robust, reduces overfitting, good accuracy","Harder to interpret, slower inference","Handles both, robust with mixed types",High - some built-in handling or imputation
Support Vector Machine (SVM),"High F/N, clear margin between classes","Large datasets (slow), overlapping classes",Low,"Slow (O(n²·p) to O(n³), kernel dependent)","Effective in high dimensions, robust with kernel trick","Hard to interpret, sensitive to parameter tuning","Numerical preferred, needs encoding for categorical",Low - requires complete cases or imputation
K-Nearest Neighbors (KNN),"Low dimensional, locally linear boundaries",High dimensional (curse of dimensionality),Low,"Fast (O(1) train), Slow (O(n·p) inference)","Simple, no training phase, intuitive","Memory-intensive, poor with high dimensions, all features equally weighted unless scaled or weighted",Prefers numerical; categorical must be encoded,Low - requires imputation
Naive Bayes,"Text classification, conditional independence",Correlated features,Medium,Very Fast (O(n·p)),"Fast, works well with text data","Assumes feature independence, low flexibility","Very good with categorical (esp. text, word counts)",High - handles missing values well
"Gradient Boosting (e.g., XGBoost)","Complex patterns, structured/tabular data",Small data with high F/N (risk of overfitting),Low,"Slow-Medium (O(m·n·log(n)), m = number of trees)","High accuracy, handles non-linearity","Complex, prone to overfitting without tuning","Handles both, often requires encoding",Medium - some models like LightGBM handle natively
Neural Networks,"Large datasets, high F/N, image/audio/text data","Small datasets, interpretability-critical tasks",Very Low,"Slow (O(n·p·d), depends on architecture depth d)","Flexible, powerful, state-of-the-art performance","Requires lots of data, hard to tune and explain","Needs numerical input, categorical must be encoded",Low - needs complete input or special handling
